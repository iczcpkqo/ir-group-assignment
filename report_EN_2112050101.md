## IMPLEMENTATION

### Index

The indexing procedure is one of the core features provided by Lucene. The diagram below illustrates the use of the indexing process and classes. The IndexWriter is the most critical and central component of the indexing process.

==索引过程配图==

#### Analyzer

We tried many of the different types of analyzers provided by the Lucene library such as the StandardAnalyzer, EnglishAnalyzer and the CustomAnalyzer. However the best score we received came from using the EnglishAnalyzer. The EnglishAnalyzer uses the StandardTokenizer, EnglishFilter, PorterStemFilter for stemming, LowerCaseFilter.

##### Standard Analyzer

The standard parser is the default parser, used if no parser is specified. It provides syntax-based markup and is used for most languages. It’s the most used analyzer and can be used to recognize URLs and emails too. It also removes stop words and lowercases the generated tokens. 
As the dataset used in this paper is news articles, the standard parser is not the best.

##### English Analyzer

This is one of the languages analyzer of Lucene. It consists of StandardTokenizer, StandardFilter, EnglishPossessiveFilter, LowerCaseFilter, StopFilter, and PorterStemFilter.
As the dataset used in this paper is a news article, the type of content is linguistic, and the information is mainly composed of English words, so the English analyzer is more suitable for the dataset used in this paper than the standard analyzer.
As the pause words of the English analyzer can be customized, we can optimize the use of the analyzer by configuring the pause words of the analyzer. When optimizing the stop words, we used the default stop words as a baseline and compared the search scores after gradually increasing the stop word coverage to see if the added stop words were effective.
After several experiments, we found that stop words did not always improve the English language analyzer, and as the number of stop words increased, optimizing the quality of the stop word lexicon improved the search results less and less. We, therefore, stopped optimizing the stop-word lexicon once it reached 696 English words in size.

#### Creating index

The final step in the indexing phase is to store the parsed data set in an index file according to a particular structure. When designing the index structure, we need to consider whether this structure will facilitate the search. It is usually better to store the different contents as separate fields in the index rather than having all the contents of an article as one field. After we have distinguished the different fields, we can then set the weight of the search results on the different fields.
The weights we set are as follows:

==我们在不同字段上使用的权重（表格）Begin==

| Field    | Weighting |
| -------- | --------- |
| xx       |           |
| text     |           |
| pe       |           |
| doc      |           |
| docno    |           |
| headline |           |
| pub      |           |
| tp       |           |

(Financial Times Limited (1991, 1992, 1993, 1994))


| Field    | Weighting |
| -------- | --------- |
| text     |           |
| footnote |           |
| summary  |           |
| doc      |           |
| usDept   |           |
| supplem  |           |
| docno    |           |

(Federal Register (1994))


| Field  | Weighting |
| ------ | --------- |
| doc    |           |
| docNo  |           |
| phrase |           |
| abs    |           |
| ht     |           |
| h3     |           |
| h4     |           |
| f      |           |
| text   |           |

(Foreign Broadcast Information Service (1996))

| Field    | Weighting |
| -------- | --------- |
| xx       |           |
| text     |           |
| pe       |           |
| doc      |           |
| docno    |           |
| headline |           |
| pub      |           |
| tp       |           |

(Los Angeles Times (1989, 1990))

==我们在不同字段上使用的权重（表格）END==

### Search

==搜索过程配图==

#### Parsing the Topic

The content structure of the subject article is analysed before the search is carried out, then a model is created for it, and the content to be searched is parsed out before it can be searched in the index.
The fields for each topic and their corresponding meanings are as follows:

| Field | Description | Name of the attribute in the model |
| ----- | ----------- | ---------------- |
| num   |     Number    | queryNumber      |
| title | Title        | queryTitle       |
| desc  | Description | description      |
| narr  |narrative       | narrative        |

#### Querying

##### BooleanQuery

Boolean Query is a Query that gets its results by combining booleans from different queries. Different boosts can be configured for different queries. a BoostQuery is a query that can be boosted to increase its score relative to By configuring different boosts for different query fields. Our queries are progressively more effective.

Our current boost configuration for the different fields reads as follows:
==May need updating==

| Field       | Property name           | boost |
| ----------- | ---------------- | ----- |
| narrative   | narrativeQuery   | 1.4f  |
| title       | titleQuery       | 4f    |
| description | descriptionQuery | 2.5f  |


##### QueryParser

The QueryParser is an interpreter that parses query strings into Query objects. It provides utilities for converting text input into objects. The key method in QueryParser is parse(String). If we want more control over how the search is performed, we can create Query objects directly without using QueryParser, but this would be a much more complex process. [3]
We use both the QueryParser, the English analyzer and the fields in the stopword analyzer topic to perform the search. However, the search results are not the best.

##### MultiFieldQueryParser

MultiFieldQueryParser inherits from QueryParser, so MultiFieldQueryParser has similar features to QueryParser in that it not only has the flexibility of QueryParser but can also build queries that search multiple fields, so we think MultiFieldQueryParser is a better choice than QueryParser. 
We enhance the search by configuring different fields and the boosts corresponding to the different fields. 
The fields we configure are == "header" and "body"==. After configuring these fields, our search scores improved.
We configured the boost for the fields as:

| Field  | boost |
| ------ | ----- |
| header | 0.2f  |
| body   | 0.8f  |

#### Similarity

Similarity defines the components of Lucene scoring. Similarity determines how Lucene weights terms, and Lucene interacts with this class at both index-time and query-time.

##### BM25Similarity

BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.[4]

![bm25](../../../../../../../../../../../../resource/image/bm25.jpg)==插入BM25图片==


##### LMJelinekMercerSimilarity

Language model based on the Jelinek-Mercer smoothing method. The model has a single parameter, λ. The optimal value depends on both the collection and the query. The optimal value is around 0.1 for title queries and 0.7 for long queries. Values should be between 0 (exclusive) and 1 (inclusive). Values near zero act score more like a conjunction (coordinate level matching), whereas values near 1 behave the opposite (more like pure disjunction). [5]
We configured the model with the following parameters, and the experimental search results obtained were not the best.
$$
lambda = 0.5f
$$

##### LMSimilarity

Abstract superclass for language modeling Similarities. It contains the following internal classes.[6]

1. LMSimilarity.LMStats， which defines a new statistic, the probability that the collection language model generates the current term;
2. LMSimilarity.CollectionModel，which is a strategy interface for object that compute the collection language model `p(w|C)`;
3. LMSimilarity.DefaultCollectionModel，an implementation of the former, that computes the term probability as the number of occurrences of the term in the collection, divided by the total number of tokens.[8]

We instantiate an LMJelinekMercerSimilarity class by passing an instance of LMSimilarity.DefaultCollectionModel as a parameter to the constructor of an LMJelinekMercerSimilarity.

##### DFRSimilarity

The DFR scoring formula is composed of three separate components: the basic model, the aftereffect and an additional normalization component, represented by the classes BasicModel, AfterEffect and Normalization, respectively. The names of these classes were chosen to match the names of their counterparts in the Terrier IR engine.[7]

To construct a DFRSimilarity, you must specify the implementations for all three components of DFR:

BasicModel: Basic model of information content:
  BasicModelBE: Limiting form of Bose-Einstein
  BasicModelG: Geometric approximation of Bose-Einstein
  BasicModelP: Poisson approximation of the Binomial
  BasicModelD: Divergence approximation of the Binomial
  BasicModelIn: Inverse document frequency
  BasicModelIne: Inverse expected document frequency [mixture of Poisson and IDF]
  BasicModelIF: Inverse term frequency [approximation of I(ne)]

AfterEffect: First normalization of information gain:
  AfterEffectL: Laplace's law of succession
  AfterEffectB: Ratio of two Bernoulli processes
  AfterEffect.NoAfterEffect: no first normalization

Normalization: Second (length) normalization:
  NormalizationH1: Uniform distribution of term frequency
  NormalizationH2: term frequency density inversely related to length
  NormalizationH3: term frequency normalization provided by Dirichlet prior
  NormalizationZ: term frequency normalization provided by a Zipfian relation
  Normalization.NoNormalization: no second normalization

Note that qtf, the multiplicity of term-occurrence in the query, is not handled by this implementation.


## Results

### EnglishAnalyzer

==A graph showing the performance using Same **EnglishAnalyzer**, but different Similarities, then summarising it and saying which one is the best==

### StandarAnalyzer
==A graph showing the performance using Same **StandarAnalyzer**, but different Similarities, then summarising it and saying which one is the best==

### CustomAnalyzer
==A graph showing the performance using Same **CustomAnalyzer**, but different Similarities, then summarising it and saying which one is the best==


* [1] Robertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., & Gatford, M. (1995). Okapi at TREC-3. Nist Special Publication Sp, 109, 109.


* [2] page 11 A query can be boosted to increase it's score relative to other subqueries. 
https://riptutorial.com/Download/lucene.pdf

* [3] Lucene 4 Cookbook by Edwood Ng (Author), Vineeth Mohan (Author) https://www.amazon.com/Lucene-4-Cookbook-Edwood-Ng/dp/1782162283

* [4] bm25 https://en.wikipedia.org/wiki/Okapi_BM25

* [5] Zhai, C., & Lafferty, J. (2017, August). A study of smoothing methods for language models applied to ad hoc information retrieval. In ACM SIGIR Forum (Vol. 51, No. 2, pp. 268-276). New York, NY, USA: ACM.

* [6] LMSimilarity https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/LMSimilarity.html


* [7] Amati, G., & Van Rijsbergen, C. J. (2002). Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS), 20(4), 357-389.
* [8] https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/LMSimilarity.html
