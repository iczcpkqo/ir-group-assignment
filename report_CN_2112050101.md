1. Novelty and Appropriateness of the Proposed Approach
   > 拟议方法的新颖性和适当性
   * 有哪些
   * 不同用处
   * 当前情况
   * 哪个最好
   * 改进

2. Technical Soundness
   > 技术的合理性
   * 有哪些
   * 不同用处
   * 当前情况
   * 哪个最好
   * 改进

3. Clarity of Presentation, Structure, Grammar/Spelling, Style etc.
   > 演示文稿的清晰性、结构、语法/拼写、风格等。

4. Results Presentation
   > 结果展示

5. Honest, open reflection upon the success and limitations of the proposed approach
   > 对拟议方法的成功和局限性进行诚实、公开的思考

6. Adequacy of Bibliography
   > 书目的充分性

## IMPLEMENTATION

### Index

索引过程是Lucene提供的核心功能之一。 下图说明了索引过程和类的使用。 IndexWriter是索引过程中最重要和最核心的组件。

==索引过程配图==

#### Analyzer

We tried many of the different types of analyzers provided by the Lucene library such as the StandardAnalyzer, EnglishAnalyzer and the CustomAnalyzer. However the best score we received came from using the EnglishAnalyzer. The EnglishAnalyzer uses the StandardTokenizer, EnglishFilter, PorterStemFilter for stemming, LowerCaseFilter.

##### Standard Analyzer

standard分析器是默认分析器，如果没有指定分析器，则使用该分析器。它提供基于语法的标记，并且用于大多数语言。 It’s the most used analyzer and can be used to recognize URLs and emails too. It also removes stop words and lowercases the generated tokens. 
由于在本文中是使用的数据集是新闻类文章，因此标准分析器并不是最好的分析器。

##### English Analyzer

This is one of the languages analyzer of Lucene. It consists of StandardTokenizer, StandardFilter, EnglishPossessiveFilter, LowerCaseFilter, StopFilter, and PorterStemFilter.
由于在本文中使用的数据集是新闻文章，文章内容的类型为语言类内容，其信息的构成主要为英文词汇，因此使用英文分析器相比于标准分析器更加适合于本文中所使用的数据集。
由于英文分析器的停顿词可以进行自定义，因此我们可以通过配置该分析器的停顿词优化分析器的使用效果。在优化停顿词时，我们以默认停顿词作为基线，通过对比逐步增加停顿词覆盖范围后的搜索得分了解添加的停顿词是否有效。
通过多次实验后我们发现，停顿词并不总是能够提升英文分析器的效果，并且随着停顿词数量的增加，通过优化停顿词词库的质量，对搜索结果的提升越来越小。因此我们在停顿词词库的规模达到696个英文单词后停止了对停顿词词库的优化。

#### 创建索引

索引阶段的最后一个步骤是将解析后的数据集按照一定的结构保存在索引文件中。 在设计索引的结构是，我们需要考虑的是这种结构是否会为搜索带来便利，通常将文章中的不同内容分别作为不同的字段存放在索引中是更好的选择，而不是将一篇文章的内容作为一个字段。因为在我们区分了不同的字段后，我们就可以设置在不同字段上搜索结果的权重。
我们设置的权重为：

==我们在不同字段上使用的权重（表格）Begin==

| 字段     | 权重 |
| -------- | ---- |
| xx       |      |
| text     |      |
| pe       |      |
| doc      |      |
| docno    |      |
| headline |      |
| pub      |      |
| tp       |      |

(Financial Times Limited (1991, 1992, 1993, 1994))


| 字段     | 权重 |
| -------- | ---- |
| text     |      |
| footnote |      |
| summary  |      |
| doc      |      |
| usDept   |      |
| supplem  |      |
| docno    |      |

(Federal Register (1994))


| 字段   | 权重 |
| ----   | ---- |
| doc    |      |
| docNo  |      |
| phrase |      |
| abs    |      |
| ht     |      |
| h3     |      |
| h4     |      |
| f      |      |
| text   |      |

(Foreign Broadcast Information Service (1996))

| 字段     | 权重 |
| ----     | ---- |
| xx       |      |
| text     |      |
| pe       |      |
| doc      |      |
| docno    |      |
| headline |      |
| pub      |      |
| tp       |      |

(Los Angeles Times (1989, 1990))

==我们在不同字段上使用的权重（表格）END==

### Search

==搜索过程配图==

#### 解析主题

在进行搜索前需要先分析主题文章的内容结构，然后将为其创建模型，然后将需要搜索的内容解析出来然后才能在索引中进行搜索。
每篇主题的字段和对应的含义如下：

| 字段  | 描述     | 在模型中的属性名 |
| ----- | -------- | ---------------- |
| num   | 编号     | queryNumber      |
| title | 标题     | queryTitle       |
| desc  | 描述     | description      |
| narr  | 旁白     | narrative        |

#### Querying

##### BooleanQuery

Boolean Query 是一个通过使用不同queries的布尔组合获得查询结果的Query。在不同的query中可以配置不同的boost。BoostQuery is a query can be boosted to increase it's score relative to other subqueries. 通过对不同查询字段配置不同的boost，我们的查询效果逐步提升。

我们目前不同字段的boost配置内容如下：
==可能需要更新==
| 字段        | 属性名           | boost |
| ----------- | ---------------- | ----- |
| narrative   | narrativeQuery   | 1.4f  |
| title       | titleQuery       | 4f    |
| description | descriptionQuery | 2.5f  |


##### QueryParser

QueryParser 是一个将查询字符串解析为 Query 对象的解释器。它提供了将文本输入转换为对象的实用程序。QueryParser 中的关键方法是 parse(String)。如果您想更多地控制搜索的执行方式，可以不使用 QueryParser 直接创建 Query 对象，但这将是一个复杂得多的过程。[3]
我们同时使用QueryParser，英文分析器和停顿词分析主题中的字段进行搜索，然而搜索结果并不是最好的。

##### MultiFieldQueryParser

MultiFieldQueryParser继承自QueryParser，因此MultiFieldQueryParser具备了于QueryParser相似的特性，它不仅具备QueryParser的灵活性，还可以构建用于搜索多个字段的查询，因此我们认为MultiFieldQueryParser是一个比QueryParser更好的选择。因为我们可以通过配置不同的字段和不同字段对应的boost来提升搜索效果。我们通过配置=="header" 和 "body"==提升了搜索效果。
我们配置的字段的boost为：

| 字段   | boost |
| ------ | ----- |
| header | 0.2f  |
| body   | 0.8f  |

#### Similarity

Similarity defines the components of Lucene scoring. Similarity determines how Lucene weights terms, and Lucene interacts with this class at both index-time and query-time.

##### BM25Similarity

BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.[4]
==插入BM25图片==


##### LMJelinekMercerSimilarity

Language model based on the Jelinek-Mercer smoothing method. The model has a single parameter, λ. The optimal value depends on both the collection and the query. The optimal value is around 0.1 for title queries and 0.7 for long queries. Values should be between 0 (exclusive) and 1 (inclusive). Values near zero act score more like a conjunction (coordinate level matching), whereas values near 1 behave the opposite (more like pure disjunction). [5]
我们通过配置该模型的参数如下，获得的实验搜索效果并不是最好的。
$$
lambda = 0.5f
$$

##### LMSimilarity

Abstract superclass for language modeling Similarities. 它包含如下内部类：[6]

1. LMSimilarity.LMStats，它定义了一个新的统计量，即集合语言模型生成当前术语的概率；
2. LMSimilarity.CollectionModel，这是一个用于计算集合语言模型的对象的策略接口p(w|C)；
3. LMSimilarity.DefaultCollectionModel，前者的一种实现，它计算术语概率，即术语在集合中出现的次数除以标记总数。

我们通过将一个LMSimilarity.DefaultCollectionModel的实例作为参数传入一个LMJelinekMercerSimilarity的构造函数中，从而实例化一个LMJelinekMercerSimilarity类.

##### DFRSimilarity

The DFR scoring formula is composed of three separate components: the basic model, the aftereffect and an additional normalization component, represented by the classes BasicModel, AfterEffect and Normalization, respectively. The names of these classes were chosen to match the names of their counterparts in the Terrier IR engine.[7]

To construct a DFRSimilarity, you must specify the implementations for all three components of DFR:

BasicModel: Basic model of information content:
  BasicModelBE: Limiting form of Bose-Einstein
  BasicModelG: Geometric approximation of Bose-Einstein
  BasicModelP: Poisson approximation of the Binomial
  BasicModelD: Divergence approximation of the Binomial
  BasicModelIn: Inverse document frequency
  BasicModelIne: Inverse expected document frequency [mixture of Poisson and IDF]
  BasicModelIF: Inverse term frequency [approximation of I(ne)]

AfterEffect: First normalization of information gain:
  AfterEffectL: Laplace's law of succession
  AfterEffectB: Ratio of two Bernoulli processes
  AfterEffect.NoAfterEffect: no first normalization

Normalization: Second (length) normalization:
  NormalizationH1: Uniform distribution of term frequency
  NormalizationH2: term frequency density inversely related to length
  NormalizationH3: term frequency normalization provided by Dirichlet prior
  NormalizationZ: term frequency normalization provided by a Zipfian relation
  Normalization.NoNormalization: no second normalization

Note that qtf, the multiplicity of term-occurrence in the query, is not handled by this implementation.


## Results

### EnglishAnalyzer

==一个图，如中展示使用EnglishAnalyzer，和不同Similarity的表现，然后总结一下，然后说一下最好的是哪一个==

### StandarAnalyzer
==一个图，如中展示使用StandarAnalyzer，和不同Similarity的表现，然后总结一下，然后说一下最好的是哪一个==

### CustomAnalyzer
==一个图，如中展示使用CustomAnalyzer，和不同Similarity的表现，然后总结一下，然后说一下最好的是哪一个==


* BM25Similarity
Robertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., & Gatford, M. (1995). Okapi at TREC-3. Nist Special Publication Sp, 109, 109.


* 11页 A query can be boosted to increase it's score relative to other subqueries. 
https://riptutorial.com/Download/lucene.pdf

* [3] QueryParser
Lucene 4 Cookbook by Edwood Ng (Author), Vineeth Mohan (Author)
https://www.amazon.com/Lucene-4-Cookbook-Edwood-Ng/dp/1782162283

* [4] bm25
https://en.wikipedia.org/wiki/Okapi_BM25

* [5] LMJelinekMercerSimilarity
Zhai, C., & Lafferty, J. (2017, August). A study of smoothing methods for language models applied to ad hoc information retrieval. In ACM SIGIR Forum (Vol. 51, No. 2, pp. 268-276). New York, NY, USA: ACM.

* [6] LMSimilarity
https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/LMSimilarity.html


* [7] DFRSimilarity
Amati, G., & Van Rijsbergen, C. J. (2002). Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS), 20(4), 357-389.
